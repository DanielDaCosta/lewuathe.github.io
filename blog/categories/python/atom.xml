<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python | The first cry of Atom]]></title>
  <link href="http://lewuathe.github.io/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://lewuathe.github.io/"/>
  <updated>2014-09-29T21:37:08+09:00</updated>
  <id>http://lewuathe.github.io/</id>
  <author>
    <name><![CDATA[Kai Sasaki]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Logistics Regression With Sparse Vector]]></title>
    <link href="http://lewuathe.github.io/blog/2014/03/09/logistics-regression-with-sparse-vector/"/>
    <updated>2014-03-09T19:54:25+09:00</updated>
    <id>http://lewuathe.github.io/blog/2014/03/09/logistics-regression-with-sparse-vector</id>
    <content type="html"><![CDATA[<p>In my project I have to develop a model that is capable of predicting the count of page view
from sparse vector data such as</p>

<p><code>
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
</code></p>

<p>However I found it was difficult to make a model which can be applied to practical use case. From sparse vector, my current model
doesn&rsquo;t look trained sufficiently. I used <a href="http://scikit-learn.org/stable/index.html">scikit-learn</a> in python.</p>

<!-- more -->


<p>My current code is below.</p>

<p>```python</p>

<h1>&ndash;<em>&ndash; coding: utf-8 &ndash;</em>&ndash;</h1>

<p>import os
import sys
import numpy as np
import random
from sklearn import svm</p>

<p>if <strong>name</strong> == &ldquo;<strong>main</strong>&rdquo;:</p>

<pre><code>rand = np.random.RandomState(1234)
xs = []
ys = []
print "Train"
for i in xrange(0, 30):
    """
     x is a sparse vector generated from binomial distribution
     x has a 1 vector generally speaking
    """
    x = rand.binomial(size=100, n=1, p=0.01)
    xs.append(x)

    """
     y is obtained from gaussian distribution which has 0.7 as mean value
    """
    y = rand.normal(loc=0.7, scale=0.1)
    ys.append(y)

clf = svm.SVR()
# Train data
clf.fit(xs, ys)

print "Test"
ts = []
for i in xrange(0, 10):
    ts.append(rand.binomial(size=100, n=1, p=0.01))

print clf.predict(ts)
</code></pre>

<p>```</p>

<p>Through this process I can only get these results.</p>

<p><code>python
[ 0.7507838   0.7507838   0.75058871  0.7507838   0.75058871  0.75058871
  0.73136874  0.7507838   0.75058871  0.75058871]
</code></p>

<p>I think this result doesn&rsquo;t have valid significance for predicting. All values look the same to me!
Simple support vector machine might not be suitable to predict with sparse vector data. But I have no idea
how to make alternate model that can be constructed with sparse vectors. Is there anyone who have a good idea or
a paper which has proper algorithm for this case? I want to know the algorithm which can construct better regression
model from sparse vector. Please let me know that.</p>

<p>Thank you.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Idiomatic Python ~I Will Write With This Style~]]></title>
    <link href="http://lewuathe.github.io/blog/2014/01/21/idiomatic-mython/"/>
    <updated>2014-01-21T13:18:00+09:00</updated>
    <id>http://lewuathe.github.io/blog/2014/01/21/idiomatic-mython</id>
    <content type="html"><![CDATA[<p>I read a book, <em><a href="http://www.amazon.co.jp/Writing-Idiomatic-Python-2-7-3-Knupp-ebook/dp/B00B5KG0F8">Writing Idiomatic Python</a></em>
Although I usually write python codes, I have not paid attension to the style of these codes. By reading this book, I have noticed
that there are pythonic style in python codes. And I think it was good mind to write python code. There were many <em>Halmful</em>, <em>Idiomaric</em>
phrases about python code. So I&rsquo;d like to introduce some of them which I&rsquo;ll write in my own code.</p>

<!-- more -->


<p>And of course, all python developers should read this book!!</p>

<h2>Enumerate</h2>

<p>Usually, I write loop code like below.</p>

<p>```python
index = 0
for element in [&ldquo;Takeshi&rdquo;, &ldquo;Nobita&rdquo;, &ldquo;Masao&rdquo;]:</p>

<pre><code>print('{}:{}'.format(index, element))
index += 1
</code></pre>

<p>```</p>

<p>But it is harmful according to this book. Correctly, you should write like below.</p>

<p>```python
conteiner = [&ldquo;Takeshi&rdquo;, &ldquo;Nobita&rdquo;, &ldquo;Masao&rdquo;]
for index, element in enumerate(conteiner):</p>

<pre><code>print('{}:{}'.format(index, element)
</code></pre>

<p>```</p>

<h2>Arbitrary arguments</h2>

<p>In python, you can write arbitrary arguments with <code>*args</code> or <code>**kwargs</code>. Arbutrary arguments are useful when you
want to implement some types of API which is different by package versions. You can write like below.</p>

<p>```python
def make_api_call(a, b, c, *args, **kwargs):</p>

<pre><code>print a
print b
print c
print args
print kwargs
</code></pre>

<p>```</p>

<p>Run this</p>

<p>```python</p>

<h1>!/usr/bin/python</h1>

<p>if <strong>name</strong> == &ldquo;<strong>main</strong>&rdquo;:</p>

<pre><code>make_api_call(1, 2, 3, 4, 5, 6, name="Takeshi", age=23)
</code></pre>

<h1>&mdash;console&mdash;</h1>

<h1>1</h1>

<h1>2</h1>

<h1>3</h1>

<h1>(4, 5, 6)</h1>

<h1>{&lsquo;age&rsquo;: 23, &lsquo;name&rsquo;: &lsquo;Takeshi&rsquo;}</h1>

<p>#</p>

<p>```</p>

<h2>Avoid <em>Swallowing</em> useful exceptions</h2>

<p>In python, <code>exception</code> is common phrases used in <code>for</code> loop or etc. In addition to this,
<code>exception</code> gives you a useful information for debugging. So you should not <em>swallow</em> these exceptions
by writing bare <code>except</code> clause. If you don&rsquo;t have any idea about what type exceptions are raised from
third-party library, you should raise it again.</p>

<p>```python
import requests
def get_json_response(url):</p>

<pre><code>try:
    r = requests.get(url)
    return r.json()
except:
    raise
</code></pre>

<p>```</p>

<h2>Avoid using a temporary variables with swapping</h2>

<p>Use tuple.</p>

<p><code>python
foo = "FOO"
bar = "BAR"
(foo, bar) = (bar, foo)
</code></p>

<h2>Use <code>join</code> method. It&rsquo;s more faster</h2>

<p><code>python
result_list = ["Takeshi", "Nobita", "Masuo"]
reesult_string = " ".join(result_list)
</code></p>

<h2>Use format function to make a formatted string</h2>

<p>```python</p>

<h1>user is a dictionary</h1>

<p>def get_formatted_user_info(user):</p>

<pre><code>output = 'Nama: {user.name}, Age: {user.age}, Sex: {user.sex}'.format(user=user)
</code></pre>

<p>```</p>

<h2>Prefer <code>xrange</code> to <code>range</code></h2>

<p>Use <code>xrange</code></p>

<p>```python
for index in xrange(10000):</p>

<pre><code>print('index: {}'.format(index)
</code></pre>

<p>```</p>

<h2>Default value got from dicionary</h2>

<p>If there are <code>name</code> field in user, <code>get</code> returns <code>'Unknown'</code>.</p>

<p><code>python
username = user.get('name', 'Unknown')
</code></p>

<h2>Dictionary complehension</h2>

<p>The list complehension is well known about python context. But dictionary complehension is as important as this.</p>

<p><code>python
user_email = {user.name: user.email for user in users_list if user.email}
</code></p>

<h2>Set complehension</h2>

<p>In set syntax, you can use complehension expression.</p>

<p><code>python
users_first_names = {user.first_name for user in users}
</code></p>

<h2>Ignore unnecessary row in tuple</h2>

<p>If there are any data which is not necessary for you in tuple, ignore it with <code>_</code></p>

<p>```python
(name, age, <em>, </em>) = get_user_info(user)
if age > 20:</p>

<pre><code>output = '{name} can drink!'.format(name=name)
</code></pre>

<p>```</p>

<h2>Generator</h2>

<p>Python list comprehension is very useful, however, processing very large list will run out of memory.
In this case you should use <code>generator</code> which is iterative expression, but doesn&rsquo;t use memory.</p>

<p>```python
users = [&ldquo;Nobita&rdquo;, &ldquo;Takeshi&rdquo;, &ldquo;Masuo&rdquo;]
for i in (user.upper() for user in users if user != &ldquo;Takeshi&rdquo;):</p>

<pre><code>print(i)
</code></pre>

<h1>NOBITA</h1>

<h1>MASUO</h1>

<p>```</p>

<h2>Refer PEP8</h2>

<p>Python has standard set of formatting rule officially. It is called <strong>PEP8</strong>.
You should install this plugin your editor.</p>

<h2>Write as PEP252</h2>

<p>PEP257 is the set of rules of document formattings.</p>

<p>```python
def calculate_statistics(value_list):</p>

<pre><code>"""Return a tuple containing the mean, median and the mode of a list of integers

Arguments:
value_list -- a list of integer values

"""
</code></pre>

<p>```</p>

<h1>Last but not least</h1>

<p>I am a pythonia. With reading this book, I am able to write more pythonic code at my work scene.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[scikit-learnでCross Validation]]></title>
    <link href="http://lewuathe.github.io/blog/2013/11/09/scikit-learndecross-validation/"/>
    <updated>2013-11-09T21:56:00+09:00</updated>
    <id>http://lewuathe.github.io/blog/2013/11/09/scikit-learndecross-validation</id>
    <content type="html"><![CDATA[<p>だんだんとscikit-learnとMachine Learningに慣れてきた。
今回はCross ValidationとGrid Searchをやってみた。</p>

<h3>Cross Validation</h3>

<p>詳しいことは<a href="http://ja.wikipedia.org/wiki/%E4%BA%A4%E5%B7%AE%E6%A4%9C%E5%AE%9A">Wikipedia</a>に書いてある。
Cross Validationはモデルの妥当性を検証する方法のひとつ。一般的に開発用のデータは訓練データと検証データに分かれる。
しかし、このまま行ってしまうと折角の訓練データが減ってしまうことになる上に、訓練データの選び方によって汎化性能が下がってしまう可能性がある。
Wikipediaに書いてあるもののホールド・アウト検定がこれに当たる。一般にはこれはCross Validationにはあたらない。</p>

<p>ここに書いてあるK-分割交差検定がこれに当たる。K-分割交差検定では開発用のデータをK個に分割しK-1個を訓練用に、残りの一つを検証用に使いモデルの正当性を計算する。
これにより使える訓練データが増えると同時に、これらを訓練データを変えることにより、汎化性能を上げることができる。</p>

<p>scikit-learnで具体的にどのように行うのか書いてみた。訓練に使ったデータとしてはKaggleの<a href="http://www.kaggle.com/c/data-science-london-scikit-learn">Data Science London</a>で出されているものを用いた。</p>

<h3>SVM</h3>

<p>まずは単純にサポートベクターマシンでクラス分けをさせた時のコード</p>

<p>```python</p>

<h1>&ndash;<em>&ndash; coding: utf-8 &ndash;</em>&ndash;</h1>

<p>import os
import sys
from sklearn import svm
import numpy as np
import csv</p>

<p>if <strong>name</strong> == &ldquo;<strong>main</strong>&rdquo;:</p>

<pre><code>train_feature_file = np.genfromtxt(open("../data/train.csv", "rb"), delimiter=",", dtype=float)
train_label_file = np.genfromtxt(open("../data/trainLabels.csv", "rb"), delimiter=",", dtype=float)

train_features = []
train_labels = []
for train_feature, train_label in zip(train_feature_file, train_label_file):
    train_features.append(train_feature)
    train_labels.append(train_label)

train_features = np.array(train_features)
train_labels = np.array(train_labels)

clf = svm.SVC(C=100, cache_size=200, class_weight=None, coef0=0.0, degree=3,gamma=0.001, kernel="rbf", max_iter=-1, probability=False,random_state=None, shrinking=True, tol=0.001, verbose=False)

clf.fit(train_features, train_labels)

test_feature_file = np.genfromtxt(open("../data/test.csv", "rb"), delimiter=",", dtype=float)

test_features = []
print "Id,Solution"
i = 1
for test_feature in test_feature_file:
    print str(i) + "," + str(int(clf.predict(test_feature)[0]))
    i += 1
</code></pre>

<p>```</p>

<p>このモデルをCross Validationで検証してみる。</p>

<p>```python
def get_score(clf, train_features, train_labels):</p>

<pre><code>X_train, X_test, y_train, y_test = cross_validation.train_test_split(train_features, train_labels, test_size=0.4, random_state=0)

clf.fit(X_train, y_train)
print clf.score(X_test, y_test) 
</code></pre>

<p>```</p>

<p>cross_validation.train_test_splitは一定の割合が検証用データとなるように開発用データを分割する関数。この場合は<code>test_size=0.4</code>を指定したので、40%のデータを検証用として使うことになる。
<code>fit</code>が60%の訓練データで行うもので、scoreが残された40%のデータで検証を行いその正答率を出してくれる。これがこのモデルの、このテストデータにおける正当性となる。もちろんこれが高ければ高いほどよいが
汎化性能が高いかどうかはここからでは読み取ることができない。そのためK分割を行うことでK回の検証を行うことができる。これらのスコアを平均することで汎化性能も含めたモデルの正当性を表すことができる。</p>

<p>```python
def get_accuracy(clf, train_features, train_labels):</p>

<pre><code>scores = cross_validation.cross_val_score(clf, train_features, train_labels, cv=10)
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
</code></pre>

<p>```</p>

<p><code>cross_validation_cross_val_score</code>でこれらの検証のすべてのscoreを得ることができる。<code>cv</code>はK分割の分割の個数を指定することができる。今回は開発用のデータを10個に分割し10回の検証を行う。
scoresには10個のscoreが入ったリストが返ってくる。これの平均をAccuracyとして出している。これで汎化性能も含めたモデルの正当性を得ることができるが、モデルパラメータのチューニングを手で行う必要がある。
手で調整して、Accuracyを計算するというのは非常に手間なのでGrid Searchというアルゴリズムでこのチューニングをある程度自動化することができる。</p>

<h3>Grid Search</h3>

<p>パラメータの範囲を指定することで経験的に最適なパラメータの組を探索する方法がGrid Search。Pythonで行うには以下のように書く。</p>

<p>```python
def grid_search(train_features, train_labels):</p>

<pre><code>param_grid = [
    {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
    {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
]

clf = GridSearchCV(svm.SVC(C=1), param_grid, n_jobs=-1)
clf.fit(train_features, train_labels)
print clf.best_estimator_
</code></pre>

<p>```</p>

<p>param_gridに指定することでこの範囲を指定することができる。n_jobsに並列に計算を行うプロセス数を指定することができる。-1を指定するとコア数をデフォルト選ぶようになっている。与えられた訓練データに対してGrid Searchを行う。
時間は少しかかるが、この訓練データに対して最もスコアが高くなるようなモデルパラメータを選ぶことができる。この訓練データを実際のテストデータに使うことができる。</p>

<h3>まとめ</h3>

<p>PRMLには実際の機械学習のプロセスみたいなものがあまり細かく書かれていないのでKaggleに提出されているコードとかを実際にみて、こういったプロセスが分かるようになってきた。
あとscikit-learnが便利すぎて、機械学習の学習アルゴリズムだけじゃなくて特徴抽出とかモデルパラメータの最適化なども整備されていたことに驚いた。</p>
]]></content>
  </entry>
  
</feed>
